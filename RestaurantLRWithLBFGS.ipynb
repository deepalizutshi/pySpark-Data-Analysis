{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.memory','14g')\\\n",
    ".set('spark.driver.memory','14g')\\\n",
    ".set('spark.home','/Users/deepali/Downloads/spark-3.1.2-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25385be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "# sc=SparkContext(conf=conf)\n",
    "# spark = SparkSession(conf=conf)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.sparkHome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark=SparkContext.getOrCreate()\n",
    "spark._conf.set('spark.executor.memory','32g')\\\n",
    ".set('spark.driver.memory','32g')\\\n",
    ".set('spark.driver.maxResultsSize','0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de676d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e986dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = SparkConf(loadDefaults=False)\n",
    "print(spark.conf.get(\"spark.home\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths=['data2016.csv','data2017.csv','data2018.csv']\n",
    "data=spark.read.csv(paths,sep=',',inferSchema='True') #merge and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b467ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths=['data2016.csv','data2017.csv','data2018.csv']\n",
    "# data=spark.read.csv(paths,sep=',',inferSchema='True') #merge and create dataframe\n",
    "\n",
    "#drop extra columns:\n",
    "columns_to_drop = ['_c17', '_c18','_c19']\n",
    "data= data.drop(*columns_to_drop)\n",
    "\n",
    "#rename columns:\n",
    "data.withColumnRenamed(\"_c0\",\"ID\")\\\n",
    ".withColumnRenamed(\"_c1\",\"Name\")\\\n",
    ".withColumnRenamed(\"_c2\",\"Address\")\\\n",
    ".withColumnRenamed(\"_c3\",\"City\")\\\n",
    ".withColumnRenamed(\"_c4\",\"State\")\\\n",
    ".withColumnRenamed(\"_c5\",\"Zip\")\\\n",
    ".withColumnRenamed(\"_c6\",\"Latitude\")\\\n",
    ".withColumnRenamed(\"_c7\",\"Longitude\")\\\n",
    ".withColumnRenamed(\"_c8\",\"Summary\")\\\n",
    ".withColumnRenamed(\"_c9\",\"Serial\")\\\n",
    ".withColumnRenamed(\"_c10\",\"Number_Date\")\\\n",
    ".withColumnRenamed(\"_c11\",\"Date_Time\")\\\n",
    ".withColumnRenamed(\"_c12\",\"Score\")\\\n",
    ".withColumnRenamed(\"_c13\",\"Inspection_Type\")\\\n",
    ".withColumnRenamed(\"_c14\",\"Inspection_Time\")\\\n",
    ".withColumnRenamed(\"_c15\",\"Comment\")\\\n",
    ".withColumnRenamed(\"_c16\",\"Risk_Grade\")\n",
    "#data.printSchema()\n",
    "\n",
    "#convert to integer:\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType\n",
    "data = data.withColumn(\"_c0\",col(\"_c0\").cast(IntegerType())) \\\n",
    "    .withColumn(\"_c5\",col(\"_c5\").cast(IntegerType())) \\\n",
    "    .withColumn(\"_c6\",col(\"_c6\").cast(FloatType()))\\\n",
    "    .withColumn(\"_c7\",col(\"_c7\").cast(FloatType())) \\\n",
    "    .withColumn(\"_c9\",col(\"_c9\").cast(FloatType())) \\\n",
    "    .withColumn(\"_c12\",col(\"_c12\").cast(IntegerType())) \n",
    "\n",
    "\n",
    "\n",
    "#checking and dropping duplicates\n",
    "print('Count of rows: {0}'.format(data.count()))\n",
    "print('Count of distinct rows: {0}'.format(data.distinct().count()))\n",
    "data = data.dropDuplicates()\n",
    "\n",
    "#Drop entries with duplicates\n",
    "print('Count of ids: {0}'.format(data.count()))\n",
    "print('Count of distinct restaurant ids: {0}'.format(data.select([c for c in data.columns if c != '_c0']).distinct().count()))\n",
    "data = data.dropDuplicates(subset=[\n",
    "c for c in data.columns if c != '_c0'\n",
    "])\n",
    "\n",
    "#missing data\n",
    "data1=data.fillna( { '_c6':37.7434784244497, '_c7':4122.33581867157247 ,'_c9':41.415529404676109,'_c12':4.2170298040255946E8} )\n",
    "data1.show()\n",
    "selected_features=['_c12','_c16',]\n",
    "datatrim=data1.select(selected_features)\n",
    "datatrim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c21050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "data2 = datatrim.withColumn(\"_c16\", when(datatrim._c16 == \"None\",\"1\") \\\n",
    "                            .when(datatrim._c16 == \"Low Risk\",\"13\") \\\n",
    "                            .when(datatrim._c16 == \"Moderate Risk\",\"14\") \\\n",
    "                            .when(datatrim._c16 == \"Routine - Unscheduled\",\"11\") \\\n",
    "                            .when(datatrim._c16 == \"Food Borne Illness Investigation\",\"15\") \\\n",
    "                            .when(datatrim._c16 == \"Structural Inspection\",\"8\") \\\n",
    "                            .when(datatrim._c16 == \"Routine - Scheduled\",\"9\") \\\n",
    "                            .when(datatrim._c16 == \"Reinspection/Followup\",\"18\") \\\n",
    "                            .when(datatrim._c16 == \"Complaint Reinspection/Followup\",\"17\") \\\n",
    "                            .when(datatrim._c16 == \"Non-inspection site visit\",\"10\") \\\n",
    "                            .when(datatrim._c16 == \"Complaint\",\"12\") \\\n",
    "                            .when(datatrim._c16 == \"High Risk\",\"16\") \\\n",
    "                            .when(datatrim._c16 == \"New Construction\",\"7\") \\\n",
    "                            .when(datatrim._c16 == \"New Ownership\",\"3\") \\\n",
    "                            .when(datatrim._c16 == \"Home Environmental Assessment\",\"2\") \\\n",
    "                            .when(datatrim._c16 == \"New Ownership Followup\",\"4\") \\\n",
    "                            .when(datatrim._c16 == \"Special Event\",\"5\") \\\n",
    "                            .when(datatrim._c16 == \"Administrative or Document Review\",\"6\") \\\n",
    "                            .otherwise(datatrim._c16))\n",
    "data2= data2.withColumn(\"_c16\",col(\"_c16\").cast(IntegerType()))\n",
    "data2.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322fb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "data3=data2.withColumn(\"Safety\",lit(1))\n",
    "data3.show()\n",
    "# data_fin=data_fin.withColumn(\"Safety\",when(data_fin[\"_c12\"]>90, 0).otherwise(data_fin[\"Safety\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3eeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3\n",
    "data_fin = data3.withColumn(\"Safety\",when(data3[\"_c12\"]> 90, 0).otherwise(data3[\"Safety\"])).na.drop()\n",
    "data_fin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89eb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as ft\n",
    "import pyspark.mllib.linalg as ln\n",
    "\n",
    "\n",
    "import pyspark.mllib.regression as reg\n",
    "# features_to_keep=['_c12','_c16','_c13','Safety']\n",
    "features_to_keep=['_c16','Safety']\n",
    "data_final=data_fin.rdd.map(lambda row: [\n",
    "[e] if type(e) == int else e for e in row\n",
    "]).map(lambda row: [\n",
    "item for sublist in row for item in sublist\n",
    "]).map(lambda row: reg.LabeledPoint(\n",
    "row[0], ln.Vectors.dense(row[1:]))\n",
    ")\n",
    "data_final.take(5)\n",
    "# data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = data_final.randomSplit([0.8, 0.2])\n",
    "data_train.take(2)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4343a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train2=data_train.take(20)\n",
    "data_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(data_final)\n",
    "del(data_fin)\n",
    "del(data3)\n",
    "del(data2)\n",
    "del(data)\n",
    "del(data1)\n",
    "del(datatrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ce16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "LR_Model = LogisticRegressionWithLBFGS.train(data_train, iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    " LR_results = (\n",
    "           data_test.map(lambda row: row.label) \\\n",
    "           .zip(LR_Model \\\n",
    "                .predict(data_test\\\n",
    "                         .map(lambda row: row.features)))\n",
    "       ).map(lambda row: (row[0], row[1] * 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib.evaluation as ev\n",
    "LR_evaluation = ev.BinaryClassificationMetrics(LR_results)\n",
    "print('Area under PR: {0:.2f}'.format(LR_evaluation.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' .format(LR_evaluation.areaUnderROC))\n",
    "LR_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9483d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
