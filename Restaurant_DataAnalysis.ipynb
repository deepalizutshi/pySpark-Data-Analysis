{"cells":[{"cell_type":"code","source":["import pyspark.sql.types as typ\nfrom pyspark.sql.types import *\nfrom pyspark import SparkContext\nsc=SparkContext.getOrCreate()\nimport pandas as pd\n\n# data2016FilePath ='dbfs:/FileStore/shared_uploads/deepali.zutshi@sjsu.edu/data2016.xlsx'\n# data2017FilePath='dbfs:/FileStore/shared_uploads/deepali.zutshi@sjsu.edu/data2017.csv'\n# data2018FilePath='dbfs:/FileStore/shared_uploads/deepali.zutshi@sjsu.edu/data2018.csv'\n\npaths=['dbfs:/FileStore/shared_uploads/deepali.zutshi@sjsu.edu/data2016-2.csv','dbfs:/FileStore/shared_uploads/deepali.zutshi@sjsu.edu/data2017.csv','dbfs:/FileStore/shared_uploads/deepali.zutshi@sjsu.edu/data2018.csv']\ndata=spark.read.csv(paths,sep=',',inferSchema='True') #merge and create dataframe\n\n#drop extra columns:\ncolumns_to_drop = ['_c17', '_c18','_c19']\ndata= data.drop(*columns_to_drop)\n\n#rename columns:\ndata.withColumnRenamed(\"_c0\",\"ID\")\\\n.withColumnRenamed(\"_c1\",\"Name\")\\\n.withColumnRenamed(\"_c2\",\"Address\")\\\n.withColumnRenamed(\"_c3\",\"City\")\\\n.withColumnRenamed(\"_c4\",\"State\")\\\n.withColumnRenamed(\"_c5\",\"Zip\")\\\n.withColumnRenamed(\"_c6\",\"Latitude\")\\\n.withColumnRenamed(\"_c7\",\"Longitude\")\\\n.withColumnRenamed(\"_c8\",\"Summary\")\\\n.withColumnRenamed(\"_c9\",\"Serial\")\\\n.withColumnRenamed(\"_c10\",\"Number_Date\")\\\n.withColumnRenamed(\"_c11\",\"Date_Time\")\\\n.withColumnRenamed(\"_c12\",\"Score\")\\\n.withColumnRenamed(\"_c13\",\"Inspection_Type\")\\\n.withColumnRenamed(\"_c14\",\"Inspection_Time\")\\\n.withColumnRenamed(\"c_15\",\"Comment\")\\\n.withColumnRenamed(\"c_16\",\"Risk_Grade\").show()\n#data.printSchema()\n\n#convert to integer:\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,BooleanType,DateType\ndata = data.withColumn(\"_c0\",col(\"_c0\").cast(IntegerType())) \\\n    .withColumn(\"_c5\",col(\"_c5\").cast(IntegerType())) \\\n    .withColumn(\"_c6\",col(\"_c6\").cast(FloatType()))\\\n    .withColumn(\"_c7\",col(\"_c7\").cast(FloatType())) \\\n    .withColumn(\"_c9\",col(\"_c9\").cast(FloatType())) \\\n    .withColumn(\"_c12\",col(\"_c12\").cast(IntegerType())) \n\n#descriptive statistics on columns\nprint(\"Descriptive Statistics: \")\ncols=[\"_c6\",\"_c7\",\"_c9\",\"_c12\"]\ndescription=data.describe(cols)\ndescription.show()\n\n#Correlation between different columns:\ncorr = []\ncorr.append(data.corr('_c6', '_c7'))\ncorr.append(data.corr('_c6', '_c9'))\ncorr.append(data.corr('_c6', '_c12'))\ncorr.append(data.corr('_c7', '_c9'))\ncorr.append(data.corr('_c7', '_c12'))\ncorr.append(data.corr('_c9', '_c12'))\nprint(\"Correlations between: [Latitude:Longitude],[Latitude:InspectionID],[Latitude:InspectionScore],[Longitude:InspectionID],[Longitude:InspectionScore],[InspectionID:InspectionScore]\")\nprint(corr)\n\n#checking and dropping duplicates\nprint('Count of rows: {0}'.format(data.count()))\nprint('Count of distinct rows: {0}'.format(data.distinct().count()))\ndata = data.dropDuplicates()\n\n#Drop entries with duplicates\nprint('Count of ids: {0}'.format(data.count()))\nprint('Count of distinct restaurant ids: {0}'.format(data.select([c for c in data.columns if c != '_c0']).distinct().count()))\ndata = data.dropDuplicates(subset=[\nc for c in data.columns if c != '_c0'\n])\n\n#missing data\nprint(\"Replacing 'null/na' with the mean values: \")\ndata.fillna( { '_c6':37.7434784244497, '_c7':4122.33581867157247 ,'_c9':41.415529404676109,'_c12':4.2170298040255946E8} ).show()\n\n#restaurants in each zip code\nfrom pyspark.sql.functions import *\nprint(\"Count of restaurants in each zip code: \")\nrestaurants = data.groupBy(\"_c5\").agg(count(\"_c1\").alias(\"Total Restaurants\"))\nrestaurants.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"531e9949-2ada-40af-b779-4da6aff6bbe3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5127898a-4323-4105-94f5-7260fdbb31c3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Assignment#5","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3019129589772212}},"nbformat":4,"nbformat_minor":0}
